{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437256a0-2549-44b2-8f30-8bab8aceeb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from tkinter import Tk\n",
    "from tkinter import filedialog\n",
    "import pathlib\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "global_job_kwargs = dict(n_jobs=4, chunk_duration=\"1s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d94fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "spike_mask: boolean array indicating whether the spike is a good unit or not\n",
    "spike_seconds: time of the spike in seconds\n",
    "spike_clusters: cluster ID of the spike\n",
    "spike_positions: position of the spike in the probe\n",
    "strobe_seconds: time of the strobe in seconds\n",
    "estimated_strobe_seconds: estimated time of the strobe in seconds (artificially created by uniform interval between the first and the last strobe time stamps)\n",
    "tagged_good_units: list of good units\n",
    "tagged_mua_units: list of multi-unit activity (MUA) units\n",
    "dlc: DeepLabCut data\n",
    "events: event data\n",
    "vame_labels: VAME labels\n",
    "\n",
    "dlc tends to be bit longer than the strobe file and event file, so we need to trim it by the first index in the event file.\n",
    "'''\n",
    "\n",
    "# Open the folder selection dialog\n",
    "root = Tk()         \n",
    "root.withdraw()    \n",
    "root.attributes(\"-topmost\", True)\n",
    "\n",
    "selected_dir = pathlib.Path(filedialog.askdirectory(title=\"Select folder\"))\n",
    "base_dir = str(selected_dir)\n",
    "root.destroy()\n",
    "print(base_dir)\n",
    "\n",
    "######## load neural data ##########\n",
    "spike_mask = np.load(os.path.join(selected_dir, \"kilosort4\\\\spike_mask.npy\"))\n",
    "spike_seconds = np.load(os.path.join(selected_dir, \"kilosort4\\\\spike_seconds_adj.npy\"))[spike_mask]\n",
    "spike_clusters = np.load(os.path.join(selected_dir, \"kilosort4\\\\spike_clusters.npy\"))[spike_mask]\n",
    "spike_positions = np.load(os.path.join(selected_dir, \"kilosort4\\\\spike_positions.npy\"))[spike_mask]\n",
    "templates = np.load(os.path.join(selected_dir, \"kilosort4\\\\templates.npy\"))\n",
    "\n",
    "\n",
    "strobe_seconds = np.load(os.path.join(selected_dir, \"kilosort4\\\\strobe_seconds.npy\"))\n",
    "\n",
    "tagged_good_units = np.load(os.path.join(selected_dir, \"kilosort4\\\\tagged_good_units.npy\"))\n",
    "tagged_mua_units = np.load(os.path.join(selected_dir, \"kilosort4\\\\tagged_mua_units.npy\"))\n",
    "\n",
    "unit_label = pd.read_csv(os.path.join(selected_dir, \"kilosort4qMetrics\\\\templates._bc_unit_labels.tsv\"), sep=\"\\t\")\n",
    "\n",
    "######## load behavioral data ##########\n",
    "session_name = selected_dir.parts[-1]\n",
    "date_str = session_name.split('_')[1]\n",
    "\n",
    "# Convert MMDDYYYY to YYYY-MM-DD\n",
    "date_obj = datetime.datetime.strptime(date_str, \"%m%d%Y\")\n",
    "date_str = date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Search for matching file\n",
    "event_dir = pathlib.Path(r\"D:\\Neuropixels\\Event\\9153\")\n",
    "event_file = event_dir.glob(f\"*{date_str}*.csv\")\n",
    "events = pd.read_csv(event_file.__next__())\n",
    "events = events.drop_duplicates(subset='Index', keep='last') # remove duplicates\n",
    "\n",
    "dlc_dir = pathlib.Path(r\"D:\\Neuropixels\\DLC\\9153\")\n",
    "dlc_file = dlc_dir.glob(f\"*{date_str}*.h5\")\n",
    "dlc = pd.read_hdf(dlc_file.__next__())\n",
    "dlc.columns = dlc.columns.droplevel(0)\n",
    "dlc = dlc.loc[dlc.index >= events['Index'].iloc[0]] # trim DLC data to the first index in the event file\n",
    "\n",
    "# VAME_dir = pathlib.Path(r\"D:\\Neuropixels\\VAME\\9153\")\n",
    "# VAME_label_file = VAME_dir.glob(f\"*{date_str}\\VAME\\hmm*\\*hmm_label*.npy\")\n",
    "# vame_label_path = next(VAME_label_file, None)\n",
    "# vame_labels = np.load(vame_label_path)[events['Index'].iloc[0]:]\n",
    "\n",
    "# VAME_motif_usage_file = VAME_dir.glob(f\"*{date_str}\\VAME\\hmm*\\motif_usage*.npy\")\n",
    "# vame_motif_usage_path = next(VAME_motif_usage_file, None)\n",
    "# vame_motif_usage = np.load(vame_motif_usage_path)\n",
    "\n",
    "# VAME_community_label_file = VAME_dir.glob(f\"*{date_str}\\VAME\\hmm*\\community\\*label*.npy\")\n",
    "# vame_community_label_path = next(VAME_community_label_file, None)\n",
    "# vame_community_labels = np.load(vame_community_label_path)[events['Index'].iloc[0]:]\n",
    "\n",
    "# VAME_community_file = VAME_dir.glob(\"community_cohort\\hmm*\\cohort_community_bag.npy\")\n",
    "# vame_community_path = next(VAME_community_file, None)\n",
    "# vame_community = np.load(vame_community_path, allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Check the number of frames in each file\n",
    "start = events['Index'].iloc[0] # When the first event starts\n",
    "end = events['Index'].iloc[-1]\n",
    "print(\"First index: \", start)\n",
    "print(\"Last index: \", end)\n",
    "stim = events['Stim'].to_numpy()\n",
    "print(\"Event file length: \", len(stim))\n",
    "print(\"Strobe file length: \", len(strobe_seconds))\n",
    "\n",
    "snout_x = dlc[('Snout', 'x')].to_numpy()\n",
    "snout_y = dlc[('Snout', 'y')].to_numpy()\n",
    "print(\"DLC file length: \", len(snout_x))\n",
    "\n",
    "print(\"Estimated # of frames: \", (strobe_seconds[-1] - strobe_seconds[0]) * 89.97)\n",
    "\n",
    "# Create estimated strobe timings array by uniform interval between the first and the last strobe time stamps\n",
    "estimated_strobe_seconds = np.linspace(strobe_seconds[0], strobe_seconds[-1], len(events))\n",
    "print(\"Strobe file length(estimate): \", len(estimated_strobe_seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adccf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot strobe intervals ###\n",
    "strobe_interval = np.diff(strobe_seconds)\n",
    "max(strobe_interval), min(strobe_interval), np.mean(strobe_interval), np.std(strobe_interval)\n",
    "mean_si = np.mean(strobe_interval)\n",
    "std_si = np.std(strobe_interval)\n",
    "outlier_mask = np.abs(strobe_interval - mean_si) > 5 * std_si\n",
    "outliers = strobe_interval[outlier_mask]\n",
    "\n",
    "print(\"# of outlier strobe intervals:\", len(outliers))\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(3,1,1)\n",
    "plt.hist(strobe_interval, bins=300)\n",
    "plt.title(\"Strobe interval histogram\", fontsize=20)\n",
    "plt.ylabel(\"Counts\", fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(strobe_interval)\n",
    "plt.title(\"Strobe interval time series\", fontsize=20)\n",
    "plt.ylabel(\"Strobe interval (s)\", fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(np.sort(outliers))\n",
    "plt.title(\"Strobe interval outliers\", fontsize=20)\n",
    "plt.ylabel(\"Interval (s)\", fontsize=16)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Interpolate DLC data ###\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "# Specify body parts\n",
    "bodyparts = dlc.columns.get_level_values(0).unique().tolist()\n",
    "coords = dlc.columns.get_level_values(1).unique().tolist()[0:2]\n",
    "\n",
    "# Threshold for likelihood\n",
    "likelihood_threshold = 0.7\n",
    "\n",
    "dlc_interpolated = dlc.copy()\n",
    "\n",
    "# Process each body part\n",
    "for bp in bodyparts:\n",
    "    for axis in coords:\n",
    "        series = dlc[bp][axis]\n",
    "        likelihood = dlc[bp]['likelihood']\n",
    "        \n",
    "        # Mask low-confidence data\n",
    "        series_masked = series.copy()\n",
    "        series_masked[likelihood < likelihood_threshold] = np.nan\n",
    "        \n",
    "        # Get valid indices and values\n",
    "        valid = ~series_masked.isna()\n",
    "        x_valid = np.arange(len(series_masked))[valid]\n",
    "        y_valid = series_masked[valid]\n",
    "        \n",
    "        spline = UnivariateSpline(x_valid, y_valid, k=1, s=0)\n",
    "        interpolated = spline(np.arange(len(series_masked)))\n",
    "        series_filled = series_masked.copy()\n",
    "        series_filled[~valid] = interpolated[~valid]\n",
    "        dlc_interpolated[bp][axis] = series_filled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf81c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### xaxis: 150--1250\n",
    "### yaxis: 50--1050\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(dlc[('Snout', 'x')], dlc[('Snout', 'y')], lw=1)\n",
    "plt.xlabel('Snout x')\n",
    "plt.ylabel('Snout y')\n",
    "plt.title('Snout Trajectory')\n",
    "plt.gca().invert_yaxis()  # Optional: invert y-axis for image-like coordinates\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(dlc_interpolated[('Snout', 'x')], dlc_interpolated[('Snout', 'y')], lw=1)\n",
    "plt.xlabel('Snout x')\n",
    "plt.ylabel('Snout y')\n",
    "plt.title('Snout Trajectory (interpolated)')\n",
    "plt.gca().invert_yaxis()  # Optional: invert y-axis for image-like coordinates\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caf6dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import zscore, ttest_1samp\n",
    "from scipy.ndimage import label as nd_label\n",
    "\n",
    "# --- Align spikes to events and plot PETHs for significantly tuned neurons ---\n",
    "\n",
    "\n",
    "# Helper: Find event onsets (False to True transitions)\n",
    "def find_event_onsets(bool_array):\n",
    "    return np.where((~bool_array[:-1]) & (bool_array[1:]))[0] + 1\n",
    "\n",
    "# 1. Corner visits (any corner)\n",
    "corner_cols = ['Corner1', 'Corner2', 'Corner3', 'Corner4']\n",
    "corner_onsets = []\n",
    "for col in corner_cols:\n",
    "    onsets = find_event_onsets(events[col].values)\n",
    "    corner_onsets.append(onsets)\n",
    "corner_onsets = np.sort(np.concatenate(corner_onsets))\n",
    "\n",
    "# 2. Acceleration events (using snout, can be changed to other bodyparts)\n",
    "# Get x and y coordinates for all six body parts\n",
    "bodyparts = ['Snout', 'RF', 'LF', 'RH', 'LH', 'Tail']\n",
    "x_coords = np.stack([dlc_interpolated[(bp, 'x')].values for bp in bodyparts], axis=1)\n",
    "y_coords = np.stack([dlc_interpolated[(bp, 'y')].values for bp in bodyparts], axis=1)\n",
    "\n",
    "# Average across body parts\n",
    "mean_x = np.mean(x_coords, axis=1)\n",
    "mean_y = np.mean(y_coords, axis=1)\n",
    "\n",
    "# Calculate velocity and acceleration\n",
    "dt = np.median(np.diff(estimated_strobe_seconds))\n",
    "vx = np.gradient(mean_x, dt)\n",
    "vy = np.gradient(mean_y, dt)\n",
    "speed = np.sqrt(vx**2 + vy**2)\n",
    "acceleration = np.gradient(speed, dt)\n",
    "\n",
    "# Threshold for acceleration events (top 1%)\n",
    "accel_thresh = np.percentile(np.abs(acceleration), 99)\n",
    "accel_events = np.where(np.abs(acceleration) > accel_thresh)[0]\n",
    "\n",
    "# 3. VAME movement events (each motif transition)\n",
    "vame_transitions = np.where(np.diff(vame_labels) != 0)[0] + 1\n",
    "\n",
    "# --- Align spikes to events and compute PETHs ---\n",
    "def compute_peth(spike_times, event_times, window=(-1, 1), bin_size=0.02):\n",
    "    bins = np.arange(window[0], window[1] + bin_size, bin_size)\n",
    "    all_counts = []\n",
    "    for et in event_times:\n",
    "        rel_spikes = spike_times - et\n",
    "        counts, _ = np.histogram(rel_spikes, bins)\n",
    "        all_counts.append(counts)\n",
    "    return bins[:-1], np.array(all_counts)\n",
    "\n",
    "# Example: test all good units for tuning to corner visits\n",
    "window = (-1, 1)\n",
    "bin_size = 0.02\n",
    "significant_units = []\n",
    "peths = []\n",
    "peths_all = []  # Store all trial PETHs for each unit\n",
    "for unit in tagged_mua_units:\n",
    "    unit_spike_times = spike_seconds[spike_clusters == unit]\n",
    "    bins, counts = compute_peth(unit_spike_times, estimated_strobe_seconds[corner_onsets], window, bin_size)\n",
    "    baseline = counts[:, :int(abs(window[0])/bin_size)]  # pre-event\n",
    "    response = counts[:, int(abs(window[0])/bin_size):]  # post-event\n",
    "    # Test: mean response > baseline\n",
    "    stat, pval = ttest_1samp(response.mean(axis=1) - baseline.mean(axis=1), 0)\n",
    "    if pval < 0.01 and response.mean() > baseline.mean():\n",
    "        significant_units.append(unit)\n",
    "        peths.append(counts.mean(axis=0))\n",
    "        peths_all.append(counts)  # Store all trials\n",
    "\n",
    "# Plot PETHs for significant units\n",
    "if significant_units:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i, (unit, peth, all_counts) in enumerate(zip(significant_units, peths, peths_all)):\n",
    "        mean_peth = zscore(peth)\n",
    "        std_peth = np.std(zscore(all_counts), axis=0)\n",
    "        plt.plot(bins, mean_peth + i*3, label=f'Unit {unit}')\n",
    "        plt.fill_between(bins, mean_peth + i*3 - std_peth, mean_peth + i*3 + std_peth, alpha=0.2)\n",
    "        plt.axhline(i*3, color='k', lw=0.5)\n",
    "    plt.axvline(0, color='k', ls='--')\n",
    "    plt.xlabel('Time from event (s)')\n",
    "    plt.ylabel('Z-scored firing rate (offset for clarity)')\n",
    "    plt.title('PETHs for Significantly Tuned Units (Corner Visits)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No significantly tuned units found for corner visits.\")\n",
    "\n",
    "\n",
    "window = (-1, 1)\n",
    "bin_size = 0.02\n",
    "significant_units = []\n",
    "peths = []\n",
    "for unit in tagged_mua_units:\n",
    "    unit_spike_times = spike_seconds[spike_clusters == unit]\n",
    "    bins, counts = compute_peth(unit_spike_times, estimated_strobe_seconds[accel_events], window, bin_size)\n",
    "    baseline = counts[:, :int(abs(window[0])/bin_size)]  # pre-event\n",
    "    response = counts[:, int(abs(window[0])/bin_size):]  # post-event\n",
    "    # Test: mean response > baseline\n",
    "    stat, pval = ttest_1samp(response.mean(axis=1) - baseline.mean(axis=1), 0)\n",
    "    if pval < 0.01 and response.mean() > baseline.mean():\n",
    "        significant_units.append(unit)\n",
    "        peths.append(counts.mean(axis=0))\n",
    "\n",
    "# Plot PETHs for significant units\n",
    "if significant_units:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for i, (unit, peth) in enumerate(zip(significant_units, peths)):\n",
    "        plt.plot(bins, zscore(peth) + i*3, label=f'Unit {unit}')\n",
    "        std = np.std(peth)\n",
    "        plt.fill_between(bins, zscore(peth) + i*3 - std, zscore(peth) + i*3 + std, alpha=0.2)\n",
    "        plt.axhline(i*3, color='k', lw=0.5)\n",
    "    plt.axvline(0, color='k', ls='--')\n",
    "    plt.xlabel('Time from event (s)')\n",
    "    plt.ylabel('Z-scored firing rate (offset for clarity)')\n",
    "    plt.title('PETHs for Significantly Tuned Units (acceleration events)')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No significantly tuned units found for acceleration.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a5630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Use all unique neuron IDs from spike_clusters\n",
    "unit_ids = np.unique(spike_clusters)\n",
    "bin_size = 0.05  # seconds\n",
    "t_start = spike_seconds.min()\n",
    "t_end = spike_seconds.max()\n",
    "n_bins = int(np.ceil((t_end - t_start) / bin_size))\n",
    "\n",
    "# Build spike count matrix: shape (n_units, n_bins)\n",
    "spike_matrix = np.zeros((len(unit_ids), n_bins))\n",
    "for i, unit in enumerate(unit_ids):\n",
    "    times = spike_seconds[spike_clusters == unit]\n",
    "    bin_idx = ((times - t_start) / bin_size).astype(int)\n",
    "    bin_idx = bin_idx[(bin_idx >= 0) & (bin_idx < n_bins)]\n",
    "    np.add.at(spike_matrix[i], bin_idx, 1)\n",
    "\n",
    "# Z-score across time for each neuron\n",
    "spike_matrix_z = (spike_matrix - spike_matrix.mean(axis=1, keepdims=True)) / spike_matrix.std(axis=1, keepdims=True)\n",
    "\n",
    "# PCA\n",
    "n_components = 300  # You can adjust this number as needed\n",
    "pca = PCA(n_components=n_components)\n",
    "assembly_patterns = pca.fit_transform(spike_matrix_z.T)  # shape: (n_bins, n_components)\n",
    "\n",
    "# Plot the first few assembly activations\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(30):\n",
    "    plt.plot(assembly_patterns[:, i], label=f'Assembly {i+1}')\n",
    "plt.xlabel('Time bin')\n",
    "plt.ylabel('Assembly activation')\n",
    "plt.legend()\n",
    "plt.title('Cell Assembly Activations (PCA, all neurons)')\n",
    "plt.show()\n",
    "\n",
    "# Scree plot\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_)*100, marker='o')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance (%)')\n",
    "plt.title('Scree plot (all neurons)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Determine the number of units involved in a cell assembly (data-driven way)\n",
    "# Example for PCA, but works similarly for ICA (just use ica.mixing_)\n",
    "\n",
    "for assembly_idx in range(50):\n",
    "    weights = pca.components_[assembly_idx]  # shape: (n_units,)\n",
    "\n",
    "    # Data-driven threshold: units with absolute weights above mean + 2*std\n",
    "    abs_weights = np.abs(weights)\n",
    "    threshold = abs_weights.mean() + 2 * abs_weights.std()\n",
    "    involved_units_idx = np.where(abs_weights > threshold)[0]\n",
    "    involved_units = unit_ids[involved_units_idx]\n",
    "\n",
    "    print(f\"Number of units involved in assembly {assembly_idx+1}: {len(involved_units)}\")\n",
    "    print(\"Unit IDs:\", involved_units)\n",
    "\n",
    "    # Optional: visualize the sorted weights and threshold\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(np.sort(abs_weights)[::-1], marker='o', label='Absolute weights')\n",
    "    plt.axhline(threshold, color='r', linestyle='--', label='Threshold')\n",
    "    plt.xlabel('Unit rank')\n",
    "    plt.ylabel('Absolute component weight')\n",
    "    plt.title(f'Assembly {assembly_idx+1}: Absolute Weights and Threshold')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1b2283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "n_components = 30  # You can adjust this number as needed\n",
    "ica = FastICA(n_components=n_components, random_state=0, max_iter=1000)\n",
    "assembly_patterns_ica = ica.fit_transform(spike_matrix_z.T)  # shape: (n_bins, n_components)\n",
    "\n",
    "# Plot the first few assembly activations\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(n_components):\n",
    "    plt.plot(assembly_patterns_ica[:, i], label=f'Assembly {i+1}')\n",
    "plt.xlabel('Time bin')\n",
    "plt.ylabel('Assembly activation (ICA)')\n",
    "plt.legend()\n",
    "plt.title('Cell Assembly Activations (ICA, all neurons)')\n",
    "plt.show()\n",
    "\n",
    "# Scree plot is not meaningful for ICA, but you can inspect the mixing matrix\n",
    "for assembly_idx in range(n_components):\n",
    "    weights = ica.mixing_[:, assembly_idx]  # shape: (n_units,)\n",
    "\n",
    "    # Data-driven threshold: units with absolute weights above mean + 2*std\n",
    "    abs_weights = np.abs(weights)\n",
    "    threshold = abs_weights.mean() + 4 * abs_weights.std()\n",
    "    involved_units_idx = np.where(abs_weights > threshold)[0]\n",
    "    involved_units = unit_ids[involved_units_idx]\n",
    "\n",
    "    print(f\"Number of units involved in assembly {assembly_idx+1}: {len(involved_units)}\")\n",
    "    print(\"Unit IDs:\", involved_units)\n",
    "\n",
    "    # Optional: visualize the sorted weights and threshold\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(np.sort(abs_weights)[::-1], marker='o', label='Absolute weights')\n",
    "    plt.axhline(threshold, color='r', linestyle='--', label='Threshold')\n",
    "    plt.xlabel('Unit rank')\n",
    "    plt.ylabel('Absolute component weight')\n",
    "    plt.title(f'Assembly {assembly_idx+1}: Absolute Weights and Threshold (ICA)')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c1966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# NMF requires non-negative input, so shift spike_matrix_z to be all positive\n",
    "spike_matrix_nmf = spike_matrix_z - spike_matrix_z.min()\n",
    "\n",
    "n_top_components = 50  # You can adjust this number\n",
    "nmf = NMF(n_components=n_top_components, init='nndsvda', random_state=0, max_iter=1000)\n",
    "W = nmf.fit_transform(spike_matrix_nmf.T)  # shape: (n_bins, n_components)\n",
    "H = nmf.components_  # shape: (n_components, n_units)\n",
    "\n",
    "# Plot the first few assembly activations (W)\n",
    "plt.figure(figsize=(10, 4))\n",
    "for i in range(min(10, n_top_components)):\n",
    "    plt.plot(W[:, i], label=f'Assembly {i+1}')\n",
    "plt.xlabel('Time bin')\n",
    "plt.ylabel('Assembly activation (NMF)')\n",
    "plt.legend()\n",
    "plt.title('Cell Assembly Activations (NMF, all neurons)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot the number of involved units per assembly (nonzero weights in H)]\n",
    "involved_units_counts_nmf = np.sum(H > (0.5 * H.max(axis=1, keepdims=True)), axis=1)\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(1, n_top_components), involved_units_counts_nmf[:-1], marker='o')\n",
    "plt.xlabel('Assembly (NMF component) index')\n",
    "plt.ylabel('Number of involved units')\n",
    "plt.title('Number of units involved in each NMF assembly')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae14a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 30))\n",
    "for i in range(n_top_components-1):\n",
    "    plt.subplot(10, 5, i+1)\n",
    "    sorted_weights = np.sort(H[i])[::-1]\n",
    "    plt.plot(sorted_weights, marker='o')\n",
    "    plt.axhline(H.max(axis=1, keepdims=True)[i]/2, color='r', linestyle='--', label='Threshold')\n",
    "    plt.title(f'NMF Component {i+1}')\n",
    "    plt.xlabel('Unit rank')\n",
    "    plt.ylabel('Weight')\n",
    "    plt.tight_layout()\n",
    "plt.suptitle('Sorted NMF Weights for First 10 Components', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2555bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(dlc_interpolated[('Snout', 'x')], dlc_interpolated[('Snout', 'y')], s=0.15, color='black', alpha=0.2, label='DLC Snout Trajectory')\n",
    "plt.title('Snout Trajectory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0725ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "tagged_units = np.concatenate([tagged_good_units, tagged_mua_units]).squeeze()\n",
    "# Analyze what happened around the time that tagged units fired\n",
    "\n",
    "# Choose a window around each spike (e.g., +/- 1 second)\n",
    "peri_window = 1.0  # seconds\n",
    "frame_rate = 90  # Hz\n",
    "n_bins = int(2 * peri_window * frame_rate) + 1\n",
    "peri_time_axis = np.linspace(-peri_window, peri_window, n_bins)\n",
    "\n",
    "# For each tagged unit, extract peri-event speed and VAME motif\n",
    "for unit in tagged_units:\n",
    "    peri_speed = []\n",
    "    unit_spike_times = spike_seconds[spike_clusters == unit]\n",
    "    for st in unit_spike_times:\n",
    "        # Find the closest strobe timing and its index to the spike time\n",
    "        idx = np.argmin(np.abs(estimated_strobe_seconds - st))\n",
    "        closest_strobe_time = estimated_strobe_seconds[idx]\n",
    "\n",
    "        # Take 90 frames before and after that index (total 181 frames)\n",
    "        peri_idx = np.arange(idx - 90, idx + 91)\n",
    "        # Bounds check: skip if peri_idx goes out of bounds\n",
    "        if peri_idx[0]-1 < 0 or peri_idx[-1]-1 >= len(speed):\n",
    "            continue\n",
    "        peri_speed_arr = speed[peri_idx-1]\n",
    "        peri_speed.append(peri_speed_arr)\n",
    "    peri_speed = np.array(peri_speed, dtype=object)\n",
    "\n",
    "    # plot peri-spike speed (mean across all spikes)\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    peri_speed_stack = np.vstack(peri_speed)\n",
    "    plt.subplot(1,5,1)\n",
    "    plt.plot(peri_time_axis, np.nanmean(peri_speed_stack, axis=0), color='gray', alpha=0.8)\n",
    "    plt.xlabel('Time from spike (s)')\n",
    "    plt.ylabel('Speed')\n",
    "    plt.ylim(100,800)\n",
    "    plt.title(f'Unit #{unit}: Mean speed around tagged unit spikes')\n",
    "\n",
    "    # Bin the arena and calculate firing rate heatmap\n",
    "    fired_coords = []\n",
    "    for spike_time in unit_spike_times:\n",
    "        idx = np.argmin(np.abs(estimated_strobe_seconds - spike_time))\n",
    "        timegap = np.min(np.abs(estimated_strobe_seconds - spike_time))\n",
    "        if timegap > 0.02:\n",
    "            continue\n",
    "        snout_x_pos = snout_x[idx]\n",
    "        snout_y_pos = snout_y[idx]\n",
    "        fired_coords.append((snout_x_pos, snout_y_pos))\n",
    "\n",
    "    fired_coords = np.array(fired_coords)\n",
    "    # Define arena bins\n",
    "    x_bins = np.linspace(200, 1230, 15)\n",
    "    y_bins = np.linspace(40, 1070, 15)\n",
    "\n",
    "    # 2D histogram of spike positions (firing events)\n",
    "    spike_hist, xedges, yedges = np.histogram2d(\n",
    "        fired_coords[:, 0], fired_coords[:, 1], bins=[x_bins, y_bins]\n",
    "    )\n",
    "\n",
    "    # Occupancy: how many frames the animal spent in each bin\n",
    "    occupancy_hist, _, _ = np.histogram2d(\n",
    "        snout_x, snout_y, bins=[x_bins, y_bins]\n",
    "    )\n",
    "    # Convert occupancy to seconds (assuming 90 Hz)\n",
    "    occupancy_sec = occupancy_hist / frame_rate\n",
    "    # Avoid division by zero\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        firing_rate_map = np.where(occupancy_sec > 0.5, spike_hist / occupancy_sec, np.nan)\n",
    "    \n",
    "    # Plot scatter\n",
    "    plt.subplot(1,5,2)\n",
    "    plt.scatter(fired_coords[:, 0], fired_coords[:, 1], s=1, color='black', alpha=0.5)\n",
    "    plt.xlim(200, 1230)\n",
    "    plt.ylim(40, 1070)\n",
    "    plt.ylabel('Y coordinate')\n",
    "    plt.title('Snout position when the unit fired')\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.subplot(1,5,3)\n",
    "    im = plt.imshow(\n",
    "        np.rot90(firing_rate_map), \n",
    "        extent=[x_bins[0], x_bins[-1], y_bins[0], y_bins[-1]],\n",
    "        aspect='auto', cmap='coolwarm'\n",
    "    )\n",
    "    plt.xlabel('X coordinate')\n",
    "    plt.title('Firing Rate Heatmap')\n",
    "    plt.colorbar(im, label='Hz')\n",
    "    plt.title('Firing Rate Heatmap')\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "\n",
    "    #plot the position on the probe and waveform of the unit\n",
    "    plt.subplot(1,5,4)\n",
    "    shank_width = 24       # in microns\n",
    "    shank_length = -5000   # in microns\n",
    "    shank_spacing = 250    # center-to-center in microns\n",
    "    n_shanks = 4\n",
    "\n",
    "    # Draw 4 shanks\n",
    "    for i in range(n_shanks):\n",
    "        # Center positions:  (so everything is centered at x=0)\n",
    "        center_x = (i-3) * shank_spacing\n",
    "        left = center_x - shank_width / 2\n",
    "        bottom = 0\n",
    "        rect = plt.Rectangle((left, bottom), shank_width, shank_length,\n",
    "                            linewidth=2, edgecolor='k', facecolor='none', zorder=1)\n",
    "        plt.gca().add_patch(rect)\n",
    "\n",
    "    # Configure axes\n",
    "    plt.xlabel('Lateral (Î¼m) 0 = ML:-1500Î¼m')\n",
    "    plt.ylabel('Depth (Î¼m) 0 = brain surface')\n",
    "    plt.title('Neuropixels 2.0 4-shank Probe Layout')\n",
    "    # plt.gca().set_aspect('equal')\n",
    "\n",
    "\n",
    "    unit_position = spike_positions[spike_clusters == unit]\n",
    "    plt.scatter(np.median(unit_position[:,0])-800, np.median(unit_position[:,1])-5000, s=10, c='red')\n",
    "    plt.gca().invert_xaxis()\n",
    "\n",
    "    # Plot the waveform of the unit\n",
    "    plt.subplot(1,5,5)\n",
    "    wf = templates[unit, :, :]  # [n_timepoints, n_channels]\n",
    "    min_vals = wf.min(axis=0)\n",
    "    max_vals = wf.max(axis=0)\n",
    "    abs_peaks = np.where(np.abs(min_vals) > np.abs(max_vals), min_vals, max_vals)\n",
    "    main_ch_idx = np.argmax(np.abs(abs_peaks))\n",
    "    main_peak_val = abs_peaks[main_ch_idx]\n",
    "    waveform = wf[:, main_ch_idx]\n",
    "    plt.plot(waveform, color='k')\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis for waveform\n",
    "    plt.title(f'Waveform')\n",
    "    plt.xlabel('Time (samples)')\n",
    "    plt.ylabel('Template used for waveform extraction')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(selected_dir + f\"EDA\\\\unit_{unit}_EDA.png\", dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48bd7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Get all non-tagged units\n",
    "all_units = np.unique(spike_clusters)\n",
    "tagged_units = np.concatenate([tagged_good_units, tagged_mua_units]).squeeze()\n",
    "non_tagged_units = np.setdiff1d(all_units, tagged_units)\n",
    "\n",
    "# Choose a window around each spike (e.g., +/- 1 second)\n",
    "peri_window = 1.0  # seconds\n",
    "frame_rate = 90  # Hz\n",
    "n_bins = int(2 * peri_window * frame_rate) + 1\n",
    "peri_time_axis = np.linspace(-peri_window, peri_window, n_bins)\n",
    "\n",
    "# For each tagged unit, extract peri-event speed and VAME motif\n",
    "for unit in tagged_units:\n",
    "    peri_speed = []\n",
    "    unit_spike_times = spike_seconds[spike_clusters == unit]\n",
    "    for st in unit_spike_times:\n",
    "        # Find the closest strobe timing and its index to the spike time\n",
    "        idx = np.argmin(np.abs(estimated_strobe_seconds - st))\n",
    "        closest_strobe_time = estimated_strobe_seconds[idx]\n",
    "\n",
    "        # Take 90 frames before and after that index (total 181 frames)\n",
    "        peri_idx = np.arange(idx - 90, idx + 91)\n",
    "        # Bounds check: skip if peri_idx goes out of bounds\n",
    "        if peri_idx[0]-1 < 0 or peri_idx[-1]-1 >= len(speed):\n",
    "            continue\n",
    "        peri_speed_arr = speed[peri_idx-1]\n",
    "        peri_speed.append(peri_speed_arr)\n",
    "    peri_speed = np.array(peri_speed, dtype=object)\n",
    "\n",
    "    # plot peri-spike speed (mean across all spikes)\n",
    "    plt.figure(figsize=(13, 4))\n",
    "    peri_speed_stack = np.vstack(peri_speed)\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.plot(peri_time_axis, np.nanmean(peri_speed_stack, axis=0), color='gray', alpha=0.8)\n",
    "    plt.xlabel('Time from spike (s)')\n",
    "    plt.ylabel('Speed')\n",
    "    plt.title(f'Unit #{unit}: Mean speed around tagged unit spikes')\n",
    "\n",
    "    # Bin the arena and calculate firing rate heatmap\n",
    "    fired_coords = []\n",
    "    for spike_time in unit_spike_times:\n",
    "        idx = np.argmin(np.abs(estimated_strobe_seconds - spike_time))\n",
    "        timegap = np.min(np.abs(estimated_strobe_seconds - spike_time))\n",
    "        if timegap > 0.02:\n",
    "            continue\n",
    "        snout_x_pos = snout_x[idx]\n",
    "        snout_y_pos = snout_y[idx]\n",
    "        fired_coords.append((snout_x_pos, snout_y_pos))\n",
    "\n",
    "    fired_coords = np.array(fired_coords)\n",
    "    # Define arena bins\n",
    "    x_bins = np.linspace(200, 1230, 20)\n",
    "    y_bins = np.linspace(40, 1070, 20)\n",
    "\n",
    "    # 2D histogram of spike positions (firing events)\n",
    "    spike_hist, xedges, yedges = np.histogram2d(\n",
    "        fired_coords[:, 0], fired_coords[:, 1], bins=[x_bins, y_bins]\n",
    "    )\n",
    "\n",
    "    # Occupancy: how many frames the animal spent in each bin\n",
    "    occupancy_hist, _, _ = np.histogram2d(\n",
    "        snout_x, snout_y, bins=[x_bins, y_bins]\n",
    "    )\n",
    "    # Convert occupancy to seconds (assuming 90 Hz)\n",
    "    occupancy_sec = occupancy_hist / frame_rate\n",
    "    # Avoid division by zero\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        firing_rate_map = np.where(occupancy_sec > 0.5, spike_hist / occupancy_sec, np.nan)\n",
    "    \n",
    "    # Plot scatter\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.scatter(fired_coords[:, 0], fired_coords[:, 1], s=1, color='black', alpha=0.5)\n",
    "\n",
    "    # Plot heatmap\n",
    "    plt.subplot(1,3,3)\n",
    "    im = plt.imshow(\n",
    "        np.rot90(firing_rate_map), \n",
    "        extent=[x_bins[0], x_bins[-1], y_bins[0], y_bins[-1]],\n",
    "        aspect='auto', cmap='coolwarm'\n",
    "    )\n",
    "    plt.xlabel('Snout x')\n",
    "    plt.ylabel('Snout y')\n",
    "    plt.title('Firing Rate Heatmap')\n",
    "    plt.colorbar(im, label='Hz')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25076421",
   "metadata": {},
   "source": [
    "## CEBRA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838452b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data conversion\n",
    "# Convert spike timestamps to binned firing counts using estimated_strobe_seconds as time bins\n",
    "# Assumes: spike_seconds, spike_clusters, tagged_good_units, tagged_mua_units, estimated_strobe_seconds are defined\n",
    "\n",
    "import cebra\n",
    "from cebra import CEBRA\n",
    "# Combine all units you want to include (e.g., good + MUA)\n",
    "unit_ids = unit_label[(unit_label['unitType'] == 1) | (unit_label['unitType'] == 2)]\n",
    "n_units = len(unit_ids)\n",
    "n_timebins = len(estimated_strobe_seconds)\n",
    "\n",
    "# Prepare output: (n_timebins, n_units)\n",
    "binned_counts = np.zeros((n_timebins, n_units), dtype=int)\n",
    "plt.figure()\n",
    "# For each unit, bin its spikes according to estimated_strobe_seconds\n",
    "# unit_ids is a DataFrame, so iterate over its index to get the unit IDs\n",
    "for i, unit_idx in enumerate(unit_ids.index):\n",
    "    unit = unit_idx  # The index is the unit ID\n",
    "    # Get spike times for this unit\n",
    "    unit_spike_times = spike_seconds[spike_clusters == unit]\n",
    "    # Bin the spikes: np.searchsorted finds the index in estimated_strobe_seconds where each spike belongs\n",
    "    bin_idx = np.searchsorted(estimated_strobe_seconds, unit_spike_times, side='right') - 1\n",
    "    # Remove out-of-bounds (before first or after last bin)\n",
    "    bin_idx = bin_idx[(bin_idx >= 0) & (bin_idx < n_timebins-1)]\n",
    "    # Count spikes in each bin\n",
    "    counts = np.bincount(bin_idx, minlength=n_timebins)\n",
    "    plt.plot(counts)\n",
    "    binned_counts[:, i] = counts\n",
    "\n",
    "# Now binned_counts is (n_timebins, n_units), ready for CEBRA\n",
    "print(\"Binned firing count matrix shape:\", binned_counts.shape)\n",
    "\n",
    "neural_data = binned_counts  # shape: (n_bins, n_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10e6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split data and labels (labels we use later!)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Smooth speed with a moving average of 5 bins\n",
    "window_size = 5\n",
    "speed_smooth = np.convolve(speed, np.ones(window_size)/window_size, mode='same')\n",
    "# Cap the speed at 5000\n",
    "speed_smooth = np.clip(speed_smooth, None, 2500)\n",
    "\n",
    "split_idx = int(0.8 * len(neural_data)) #suggest: 5%-20% depending on your dataset size\n",
    "train_data = neural_data[:split_idx]\n",
    "valid_data = neural_data[split_idx:]\n",
    "\n",
    "\n",
    "'''\n",
    "0: x-coordinate of the snout\n",
    "1: y-coordinate of the snout\n",
    "2: x-coordinate of the snout + y-coordinate of the snout\n",
    "3: smoothed speed(smoothing window = 5, capped at 5000 pxl/sec)\n",
    "'''\n",
    "# Stack all four arrays first, then slice for train/valid\n",
    "snout_x_all = dlc[('Snout', 'x')].to_numpy()\n",
    "snout_y_all = dlc[('Snout', 'y')].to_numpy()\n",
    "snout_xy_all = snout_x_all + snout_y_all\n",
    "speed_smooth_all = speed_smooth\n",
    "\n",
    "continuous_label = np.stack([snout_x_all, snout_y_all, snout_xy_all, speed_smooth_all], axis=1)\n",
    "train_continuous_label = continuous_label[:split_idx]\n",
    "valid_continuous_label = continuous_label[split_idx:]\n",
    "\n",
    "# VAME label as discrete label\n",
    "train_discrete_label = vame_labels[:split_idx]\n",
    "valid_discrete_label = vame_labels[split_idx:]\n",
    "\n",
    "# 1. Define a CEBRA model\n",
    "cebra_model = CEBRA(\n",
    "    model_architecture=\"offset10-model\", #consider: \"offset10-model-mse\" if Euclidean\n",
    "    batch_size=2048,\n",
    "    learning_rate=3e-4,\n",
    "    temperature=0.1,\n",
    "    max_iterations=5000, #we will sweep later; start with default\n",
    "    conditional='time', #for supervised, put 'time_delta', or 'delta'\n",
    "    output_dimension=32,\n",
    "    num_hidden_units=256,\n",
    "    distance='cosine', #consider 'euclidean'; if you set this, output_dimension min=2\n",
    "    device=\"cuda\",\n",
    "    verbose=True,\n",
    "    time_offsets=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaf7c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%mkdir saved_models\n",
    "\n",
    "params_grid = dict(\n",
    "    model_architecture=\"offset10-model\",\n",
    "    device=\"cuda\",  # or \"cpu\"\n",
    "    batch_size=2048,\n",
    "    learning_rate=3e-4,\n",
    "    max_iterations=5000,\n",
    "    temperature=[0.1, 1.0],\n",
    "    output_dimension=[3, 6, 12, 32],\n",
    "    time_offsets=[5,10,50],\n",
    "    verbose=True,\n",
    "    num_hidden_units=[32,64,256],\n",
    "    hybrid=False\n",
    ")\n",
    "\n",
    "datasets = {\"dataset1\": train_data}\n",
    "# run the grid search\n",
    "grid_search = cebra.grid_search.GridSearch()\n",
    "grid_search.fit_models(datasets, params=params_grid, models_dir=\"saved_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the results\n",
    "df_results = grid_search.get_df_results(models_dir=\"saved_models\")\n",
    "\n",
    "# Get the best model for a given dataset\n",
    "best_model, best_model_name = grid_search.get_best_model(dataset_name=\"dataset1\", models_dir=\"saved_models\")\n",
    "print(\"The best model is:\", best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the top model âœ¨\n",
    "model_path = Path(\"saved_models\") / f\"{best_model_name}.pt\"\n",
    "top_model = cebra.CEBRA.load(model_path)\n",
    "\n",
    "#transform:\n",
    "top_train_embedding = top_model.transform(train_data)\n",
    "top_valid_embedding = top_model.transform(valid_data)\n",
    "\n",
    "# plot the loss curve\n",
    "ax = cebra.plot_loss(top_model)\n",
    "\n",
    "\n",
    "# plot embeddings\n",
    "fig = cebra.integrations.plotly.plot_embedding_interactive(top_train_embedding,\n",
    "                                                           embedding_labels=train_continuous_label[:,3],\n",
    "                                                           title = \"top model - train\",\n",
    "                                                           markersize=3,\n",
    "                                                           cmap = \"rainbow\")\n",
    "fig.show()\n",
    "\n",
    "fig = cebra.integrations.plotly.plot_embedding_interactive(top_valid_embedding,\n",
    "                                                           embedding_labels=valid_continuous_label[:,3],\n",
    "                                                           title = \"top model - validation\",\n",
    "                                                           markersize=3,\n",
    "                                                           cmap = \"rainbow\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ff245",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now we are going to run our train/val. 5-10 times to be sure they are consistent!\n",
    "\n",
    "X = 5  # Number of training runs\n",
    "model_paths = []  # Store file paths\n",
    "\n",
    "for i in range(X):\n",
    "    print(f\"Training ðŸ¦“CEBRA model {i+1}/{X}\")\n",
    "\n",
    "    # Train and save model\n",
    "    cebra_train_model = cebra_model.fit(train_data)\n",
    "    tmp_file = Path(tempfile.gettempdir(), f'cebra_{i}.pt')\n",
    "    cebra_train_model.save(tmp_file)\n",
    "    model_paths.append(tmp_file)\n",
    "\n",
    "### Reload models and transform data\n",
    "train_embeddings = []\n",
    "valid_embeddings = []\n",
    "\n",
    "for tmp_file in model_paths:\n",
    "    cebra_train_model = cebra.CEBRA.load(tmp_file)\n",
    "    train_embeddings.append(cebra_train_model.transform(train_data))\n",
    "    valid_embeddings.append(cebra_train_model.transform(valid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d705ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, pairs, ids_runs = cebra.sklearn.metrics.consistency_score(\n",
    "    embeddings=train_embeddings,\n",
    "    between=\"runs\"\n",
    ")\n",
    "\n",
    "cebra.plot_consistency(scores, pairs, ids_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89bb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores, pairs, ids_runs = cebra.sklearn.metrics.consistency_score(\n",
    "    embeddings=valid_embeddings,\n",
    "    between=\"runs\"\n",
    ")\n",
    "\n",
    "cebra.plot_consistency(scores, pairs, ids_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b6166",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check the behavioral encoding\n",
    "# Define the model\n",
    "# consider changing based on search/results above\n",
    "cebra_behavior_model = CEBRA(model_architecture='offset10-model',\n",
    "                        batch_size=2048,\n",
    "                        learning_rate=3e-4,\n",
    "                        temperature=0.1,\n",
    "                        output_dimension=32,\n",
    "                        num_hidden_units=256,\n",
    "                        max_iterations=5000,\n",
    "                        distance='cosine',\n",
    "                        conditional='time_delta', #using labels\n",
    "                        device='cuda',\n",
    "                        verbose=True,\n",
    "                        time_offsets=5)\n",
    "# fit\n",
    "cebra_behavior_full_model = cebra_behavior_model.fit(neural_data, continuous_label)\n",
    "# transform\n",
    "cebra_behavior_full = cebra_behavior_full_model.transform(neural_data)\n",
    "# GoF\n",
    "gof_full = cebra.sklearn.metrics.goodness_of_fit_score(cebra_behavior_full_model,neural_data, continuous_label)\n",
    "print(\" GoF in bits - full:\", gof_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1972152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    # plot embedding\n",
    "    fig = cebra.integrations.plotly.plot_embedding_interactive(cebra_behavior_full, embedding_labels=continuous_label[:,i], title = \"CEBRA-Behavior (full)\", markersize=3, cmap = \"rainbow\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a26928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Shuffle control model:\n",
    "cebra_shuffled_model = CEBRA(model_architecture='offset10-model',\n",
    "                        batch_size=2048,\n",
    "                        learning_rate=3e-4,\n",
    "                        temperature=0.1,\n",
    "                        output_dimension=32,\n",
    "                        max_iterations=5000,\n",
    "                        num_hidden_units=256,\n",
    "                        distance='cosine',\n",
    "                        conditional='time_delta',\n",
    "                        device='cuda_if_available',\n",
    "                        verbose=True,\n",
    "                        time_offsets=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1035cb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the behavior variable and use it for training\n",
    "shuffled_pos = np.random.permutation(continuous_label)\n",
    "\n",
    "#fit, transform\n",
    "cebra_shuffled_model.fit(neural_data, shuffled_pos)\n",
    "cebra_pos_shuffled = cebra_shuffled_model.transform(neural_data)\n",
    "# GoF\n",
    "gof_full = cebra.sklearn.metrics.goodness_of_fit_score(cebra_shuffled_model, neural_data, continuous_label)\n",
    "print(\" GoF in bits - full:\", gof_full)\n",
    "# plot embedding\n",
    "fig = cebra.integrations.plotly.plot_embedding_interactive(cebra_pos_shuffled, embedding_labels=continuous_label[:,0], title = \"CEBRA-Behavior (labels shuffled)\", markersize=3, cmap = \"rainbow\")\n",
    "fig.show()\n",
    "# plot the loss curve\n",
    "ax = cebra.plot_loss(cebra_shuffled_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cfe3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Shuffle the neural data and use it for training\n",
    "shuffle_idx = np.random.permutation(len(neural_data))\n",
    "shuffled_neural = neural_data[shuffle_idx]\n",
    "#fit, transform\n",
    "cebra_shuffled_model.fit(shuffled_neural, continuous_label)\n",
    "cebra_neural_shuffled = cebra_shuffled_model.transform(shuffled_neural)\n",
    "# GoF\n",
    "gof_full = cebra.sklearn.metrics.goodness_of_fit_score(cebra_shuffled_model, shuffled_neural, continuous_label)\n",
    "print(\" GoF in bits - full:\", gof_full)\n",
    "# plot embedding\n",
    "fig = cebra.integrations.plotly.plot_embedding_interactive(cebra_neural_shuffled, embedding_labels=continuous_label[:,0], title = \"CEBRA-Behavior (neural shuffled)\", markersize=3, cmap = \"rainbow\")\n",
    "fig.show()\n",
    "# plot the loss curve\n",
    "ax = cebra.plot_loss(cebra_shuffled_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa89fc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "cebra_hybrid_model = CEBRA(model_architecture='offset10-model',\n",
    "                        batch_size=2048,\n",
    "                        learning_rate=3e-4,\n",
    "                        temperature=1.12,\n",
    "                        output_dimension=8,\n",
    "                        max_iterations=5000,\n",
    "                        num_hidden_units=64,\n",
    "                        distance='cosine',\n",
    "                        conditional='time_delta',\n",
    "                        device='cuda',\n",
    "                        verbose=True,\n",
    "                        time_offsets=5,\n",
    "                        hybrid = True)\n",
    "\n",
    "cebra_hybrid_model.fit(neural_data, continuous_label)\n",
    "cebra_hybrid = cebra_hybrid_model.transform(neural_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddadf704",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    # plot embedding\n",
    "    fig = cebra.integrations.plotly.plot_embedding_interactive(cebra_hybrid, embedding_labels=continuous_label[:,i], title = \"CEBRA-Behavior (full)\", markersize=3, cmap = \"rainbow\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd97dd5",
   "metadata": {},
   "source": [
    "### PP-seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f33e1",
   "metadata": {},
   "source": [
    "!pip install PPseq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcad5515",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
