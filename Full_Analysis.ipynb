{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1818 Full Analysis Pipeline\n",
    "\n",
    "This notebook ties together alignment (events/DLC/DA ↔ NP), LFP/CSD, spike–LFP coupling, PAC, coherence, and replay/HMM skeletons.\n",
    "\n",
    "Fill the marked placeholders (paths, strobe mappings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import welch, spectrogram, butter, filtfilt, hilbert, coherence, find_peaks\n",
    "from scipy.io import loadmat\n",
    "from tqdm.auto import tqdm\n",
    "try:\n",
    "    import cupy as cp\n",
    "    have_cupy = True\n",
    "except ImportError:\n",
    "    have_cupy = False\n",
    "try:\n",
    "    from hmmlearn import hmm\n",
    "    have_hmm = True\n",
    "except ImportError:\n",
    "    have_hmm = False\n",
    "\n",
    "import spikeinterface.full as si\n",
    "from spikeinterface.core import NumpyRecording\n",
    "\n",
    "# ---- paths (edit) ----\n",
    "spikeglx_probe_folder = Path(r\"Z:\\Koji\\Neuropixels\\1818\\1818_11202025_g0\\1818_11202025_g0_imec0\")\n",
    "spike_dir = spikeglx_probe_folder / \"kilosort4\"\n",
    "spike_times_path = spike_dir / \"spike_seconds_adj.npy\"\n",
    "spike_clusters_path = spike_dir / \"spike_clusters.npy\"\n",
    "unit_labels_path = spike_dir / \"..\" / \"kilosort4qMetrics\" / \"templates._bc_unit_labels.tsv\"\n",
    "celltype_path = spike_dir / \"unit_classification_rulebased.csv\"\n",
    "csd_memmap_path = spike_dir / \"csd_tmp.float32.bin\"\n",
    "spectrogram_meta = spikeglx_probe_folder / \"spectrogram_fullsession_meta.npz\"\n",
    "spectrogram_memmap = spikeglx_probe_folder / \"spectrogram_fullsession.dat\"\n",
    "\n",
    "event_csvs = [\n",
    "    Path(r\"Z:\\Koji\\NP_Coh3\\Recording\\Day27_1818_Clockwise_corner_2025-11-20T15_11_30.csv\"),\n",
    "    Path(r\"Z:\\Koji\\NP_Coh3\\Recording\\Day27_1818_Clockwise_licking_2025-11-20T15_11_30.csv\"),\n",
    "]\n",
    "dlc_csv = Path(r\"Z:\\Koji\\NP_Coh3\\Recording\\Day27_1818_Clockwise2025-11-20T15_50_10DLC_HrnetW32_openfield_v3Sep10shuffle2_detector_170_snapshot_160.csv\")\n",
    "da_mat = Path(r\"Z:\\Koji\\NP_Coh3\\TDT\\1818\\250919\\1818_250919_dFF.mat\")\n",
    "np_strobe_npy = spike_dir / \"strobe_signal.npy\"  # set to actual NP strobe npy\n",
    "cam_fps = 60.0\n",
    "\n",
    "# ---- strobe mapping placeholders (fill once you run alignment) ----\n",
    "# t_np = a_cam_np * t_cam + b_cam_np\n",
    "a_cam_np = None  # e.g., 1.0\n",
    "b_cam_np = None  # e.g., 0.0\n",
    "# t_np = a_da_np * t_da + b_da_np\n",
    "a_da_np = None\n",
    "b_da_np = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load behavior and DA strobe/DA signal\n",
    "events = pd.concat([pd.read_csv(p) for p in event_csvs], ignore_index=True)\n",
    "dlc = pd.read_csv(dlc_csv, header=None)\n",
    "dlc_time_cam = np.arange(len(dlc)) / cam_fps\n",
    "\n",
    "da = loadmat(da_mat)\n",
    "da_dff = None; da_strobe = None; da_fs = None\n",
    "for k in da.keys():\n",
    "    kl = k.lower()\n",
    "    if kl == 'dff':\n",
    "        da_dff = np.asarray(da[k]).squeeze()\n",
    "    if 'strobe' in kl:\n",
    "        da_strobe = np.asarray(da[k]).squeeze()\n",
    "    if kl == 'fs':\n",
    "        da_fs = float(np.asarray(da[k]).squeeze())\n",
    "np_strobe = np.load(np_strobe_npy) if np_strobe_npy.exists() else None\n",
    "print(\"Events\", events.shape, \"DLC\", dlc.shape, \"DA dFF\", None if da_dff is None else da_dff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build strobe mappings if coefficients not set\n",
    "def detect_strobe_edges(sig, fs, height=None, distance=None):\n",
    "    sig = np.asarray(sig).astype(float)\n",
    "    if height is None:\n",
    "        height = 0.5 * (sig.min() + sig.max())\n",
    "    if distance is None:\n",
    "        distance = max(1, int(0.5 * fs / cam_fps))\n",
    "    peaks, _ = find_peaks(sig, height=height, distance=distance)\n",
    "    return peaks / fs\n",
    "\n",
    "if np_strobe is not None and (a_cam_np is None or b_cam_np is None):\n",
    "    fs_np = 1000.0  # set NP strobe sampling rate correctly (e.g., 30000 for AP or 1000 for LF)\n",
    "    np_edges = detect_strobe_edges(np_strobe, fs=fs_np)\n",
    "    cam_edges = np.arange(len(dlc_time_cam)) / cam_fps\n",
    "    n_match = min(len(np_edges), len(cam_edges))\n",
    "    if n_match > 5:\n",
    "        a_cam_np, b_cam_np = np.polyfit(cam_edges[:n_match], np_edges[:n_match], 1)\n",
    "        print(f\"Fitted cam->NP: t_np = {a_cam_np:.6f}*t_cam + {b_cam_np:.6f}\")\n",
    "if np_strobe is not None and da_strobe is not None and da_fs is not None and (a_da_np is None or b_da_np is None):\n",
    "    np_edges = detect_strobe_edges(np_strobe, fs=fs_np)\n",
    "    da_edges = detect_strobe_edges(da_strobe, fs=da_fs)\n",
    "    n_match = min(len(np_edges), len(da_edges))\n",
    "    if n_match > 5:\n",
    "        a_da_np, b_da_np = np.polyfit(da_edges[:n_match], np_edges[:n_match], 1)\n",
    "        print(f\"Fitted DA->NP: t_np = {a_da_np:.6f}*t_da + {b_da_np:.6f}\")\n",
    "\n",
    "def cam_to_np_time(t_cam):\n",
    "    if a_cam_np is None or b_cam_np is None:\n",
    "        raise RuntimeError(\"Set a_cam_np/b_cam_np\")\n",
    "    return a_cam_np * np.asarray(t_cam) + b_cam_np\n",
    "\n",
    "def da_to_np_time(t_da):\n",
    "    if a_da_np is None or b_da_np is None:\n",
    "        raise RuntimeError(\"Set a_da_np/b_da_np\")\n",
    "    return a_da_np * np.asarray(t_da) + b_da_np\n",
    "\n",
    "dlc_time_np = cam_to_np_time(dlc_time_cam) if (a_cam_np is not None and b_cam_np is not None) else None\n",
    "da_time_np = da_to_np_time(np.arange(len(da_dff))/da_fs) if (da_dff is not None and a_da_np is not None) else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spikes and LFP/CSD\n",
    "spike_times = np.load(spike_times_path)\n",
    "spike_clusters = np.load(spike_clusters_path)\n",
    "unit_labels = np.loadtxt(unit_labels_path, delimiter=\"\\t\", dtype=int)\n",
    "good_units = unit_labels == 1\n",
    "celltypes = {}\n",
    "if celltype_path.exists():\n",
    "    df_ct = pd.read_csv(celltype_path)\n",
    "    celltypes = dict(zip(df_ct[\"cluster_id\"], df_ct[\"cell_type\"]))\n",
    "lfp_rec = si.read_spikeglx(spikeglx_probe_folder, stream_id=\"imec0.lf\")\n",
    "csd_rec = None\n",
    "if csd_memmap_path.exists():\n",
    "    n_frames = lfp_rec.get_num_frames(); n_channels = lfp_rec.get_num_channels(); fs = lfp_rec.get_sampling_frequency()\n",
    "    csd_mm = np.memmap(csd_memmap_path, dtype=np.float32, mode=\"r\", shape=(n_frames, n_channels))\n",
    "    csd_rec = NumpyRecording([csd_mm], sampling_frequency=fs)\n",
    "    csd_rec.set_probe(lfp_rec.get_probe()); csd_rec.set_channel_ids(lfp_rec.channel_ids)\n",
    "print(lfp_rec)\n",
    "if csd_rec: print(\"CSD memmap loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSD / Bandpower / Coherence / PAC / Coupling (from SpikeLFP_analysis)\n",
    "def compute_bandpower(rec, band, win_s=0.5, step_s=0.1, duration_s=120):\n",
    "    fs = rec.get_sampling_frequency(); n_frames_total = min(int(duration_s * fs), rec.get_num_frames())\n",
    "    win = int(win_s * fs); step = max(1, int(step_s * fs)); b,a = butter(4, [band[0]/(fs/2), band[1]/(fs/2)], btype='band')\n",
    "    powers=[]; times=[]\n",
    "    for start in tqdm(range(0, n_frames_total - win + 1, step), desc=f\"{band[0]}-{band[1]} Hz\"):\n",
    "        end = start + win; x = rec.get_traces(start_frame=start, end_frame=end); xf = filtfilt(b,a,x,axis=0); p = np.mean(xf**2, axis=0)\n",
    "        powers.append(p); times.append(start/fs)\n",
    "    return np.asarray(times), np.vstack(powers)\n",
    "\n",
    "def compute_phase_locking(spike_times_s, spike_clusters, good_units_mask, lfp_rec, band=(13,30)):\n",
    "    fs = lfp_rec.get_sampling_frequency(); b,a = butter(4, [band[0]/(fs/2), band[1]/(fs/2)], btype='band'); phases={}\n",
    "    for unit_id, good in enumerate(good_units_mask):\n",
    "        if not good: continue\n",
    "        st = spike_times_s[spike_clusters == unit_id];\n",
    "        if st.size==0: continue\n",
    "        ch=0; end_f = min(lfp_rec.get_num_frames(), int((st.max()+1)*fs)); lfp = lfp_rec.get_traces(start_frame=0, end_frame=end_f, channel_ids=[ch])[:,0]\n",
    "        phase = np.angle(hilbert(filtfilt(b,a,lfp)))\n",
    "        idx = (st*fs).astype(int); idx = idx[idx < phase.size]; phases[unit_id] = phase[idx]\n",
    "    return phases\n",
    "\n",
    "def spike_field_coherence(unit_id, spike_times_s, spike_clusters, lfp_rec, ch=0, duration_s=120, nperseg=1024, noverlap=512, max_f=200):\n",
    "    fs = lfp_rec.get_sampling_frequency(); n_frames = min(int(duration_s*fs), lfp_rec.get_num_frames())\n",
    "    st = spike_times_s[spike_clusters == unit_id]; st = st[st < duration_s]; spikes_bin = np.zeros(n_frames); idx = (st*fs).astype(int); idx = idx[idx < n_frames]; spikes_bin[idx]=1\n",
    "    lfp = lfp_rec.get_traces(start_frame=0, end_frame=n_frames, channel_ids=[ch])[:,0]\n",
    "    f,Cxy = coherence(spikes_bin, lfp, fs=fs, nperseg=nperseg, noverlap=noverlap); mask = f<=max_f; return f[mask], Cxy[mask]\n",
    "\n",
    "def field_field_coherence(lfp_rec, ch_a=0, ch_b=1, duration_s=120, nperseg=2048, noverlap=1024, max_f=200):\n",
    "    fs = lfp_rec.get_sampling_frequency(); n_frames = min(int(duration_s*fs), lfp_rec.get_num_frames()); x = lfp_rec.get_traces(start_frame=0, end_frame=n_frames, channel_ids=[ch_a,ch_b])\n",
    "    f,Cxy = coherence(x[:,0], x[:,1], fs=fs, nperseg=nperseg, noverlap=noverlap); mask = f<=max_f; return f[mask], Cxy[mask]\n",
    "\n",
    "def compute_pac(lfp_rec, phase_band=(13,30), amp_band=(30,80), duration_s=60):\n",
    "    fs = lfp_rec.get_sampling_frequency(); n_frames = min(int(duration_s*fs), lfp_rec.get_num_frames()); x = lfp_rec.get_traces(start_frame=0, end_frame=n_frames)\n",
    "    b_p,a_p = butter(4, [phase_band[0]/(fs/2), phase_band[1]/(fs/2)], btype='band'); b_a,a_a = butter(4, [amp_band[0]/(fs/2), amp_band[1]/(fs/2)], btype='band')\n",
    "    phase = np.angle(hilbert(filtfilt(b_p,a_p,x,axis=0))); amp = np.abs(hilbert(filtfilt(b_a,a_a,x,axis=0)))\n",
    "    nbins=18; bins = np.linspace(-np.pi, np.pi, nbins+1); mi = np.zeros(x.shape[1])\n",
    "    for ch in range(x.shape[1]):\n",
    "        digitized = np.digitize(phase[:,ch], bins) - 1; mean_amp = np.array([amp[digitized==b,ch].mean() for b in range(nbins)]); mean_amp /= mean_amp.sum()\n",
    "        mi[ch] = (np.log(nbins) + np.sum(mean_amp*np.log(mean_amp+1e-12))) / np.log(nbins)\n",
    "    return mi\n",
    "\n",
    "print(\"Helper functions loaded. Use as needed below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: bandpower and PAC\n",
    "bands = [(4,8),(13,30),(30,80)]\n",
    "band_results = {}\n",
    "for band in bands:\n",
    "    times,pwr = compute_bandpower(lfp_rec, band=band, win_s=0.5, step_s=0.1, duration_s=120)\n",
    "    band_results[band]=(times,pwr)\n",
    "pac_beta_gamma = compute_pac(lfp_rec, phase_band=(13,30), amp_band=(30,80), duration_s=60)\n",
    "print(\"Bandpower/PAC examples done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay decoding skeleton\n",
    "1. Choose position from DLC (`dlc_pos = dlc.iloc[:,0].values` or other column).\n",
    "2. Ensure `dlc_time_np` is set from strobe mapping.\n",
    "3. Compute tuning curves and decode in events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tuning(pos, pos_time, spikes_s, spikes_clu, good_mask, nbins=50):\n",
    "    pos = np.asarray(pos); pos_time = np.asarray(pos_time)\n",
    "    edges = np.linspace(pos.min(), pos.max(), nbins+1); centers = 0.5*(edges[:-1]+edges[1:])\n",
    "    dt = np.median(np.diff(pos_time))\n",
    "    occ,_ = np.histogram(pos, bins=edges); occ = occ*dt\n",
    "    tc={}\n",
    "    for uid, good in enumerate(good_mask):\n",
    "        if not good: continue\n",
    "        st = spikes_s[spikes_clu==uid];\n",
    "        if st.size==0: continue\n",
    "        pos_at_spikes = np.interp(st, pos_time, pos)\n",
    "        spk_counts,_ = np.histogram(pos_at_spikes, bins=edges)\n",
    "        tc[uid] = (spk_counts+1e-3)/(occ+1e-3)\n",
    "    return centers, tc\n",
    "\n",
    "def decode_position(tuning_curves, pos_bins, spike_counts, dt=0.02):\n",
    "    units = list(tuning_curves.keys()); lam = np.stack([tuning_curves[u] for u in units])\n",
    "    # spike_counts shape (n_units, n_timebins)\n",
    "    log_l = spike_counts.T[:,:,None]*np.log(lam.T[None,:,:]+1e-12) - dt*lam.T[None,:,:]\n",
    "    post = np.exp(log_l.sum(axis=1)); post /= post.sum(axis=1, keepdims=True)\n",
    "    return post, units\n",
    "\n",
    "# Example usage (uncomment, set dlc_pos):\n",
    "# dlc_pos = dlc.iloc[:,0].values\n",
    "# centers, tc = compute_tuning(dlc_pos, dlc_time_np, spike_times, spike_clusters, good_units)\n",
    "# Build spike_counts by binning spikes into dt bins during candidate events, then call decode_position(tc, centers, spike_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM on LFP bandpower (optional; requires hmmlearn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lfp_bandpower_features(lfp_rec, bands, win_s=0.5, step_s=0.1, duration_s=120):\n",
    "    fs = lfp_rec.get_sampling_frequency(); n_frames = min(int(duration_s*fs), lfp_rec.get_num_frames())\n",
    "    win = int(win_s*fs); step = max(1, int(step_s*fs)); feats=[]; times=[]\n",
    "    filters = [butter(4, [b[0]/(fs/2), b[1]/(fs/2)], btype='band') for b in bands]\n",
    "    for start in tqdm(range(0, n_frames - win + 1, step), desc=\"LFP bandpower feats\"):\n",
    "        end = start + win; x = lfp_rec.get_traces(start_frame=start, end_frame=end)\n",
    "        bp = []\n",
    "        for b,a in filters:\n",
    "            xf = filtfilt(b,a,x,axis=0); bp.append(np.mean(xf**2, axis=0))\n",
    "        feats.append(np.hstack(bp)); times.append(start/fs)\n",
    "    return np.asarray(times), np.vstack(feats)\n",
    "\n",
    "def fit_hmm_bandpower(features, n_states=3):\n",
    "    if not have_hmm:\n",
    "        raise RuntimeError(\"Install hmmlearn to use HMM\")\n",
    "    model = hmm.GaussianHMM(n_components=n_states, covariance_type='full', n_iter=100)\n",
    "    model.fit(features)\n",
    "    states = model.predict(features)\n",
    "    return model, states\n",
    "\n",
    "# Example (uncomment to run if hmmlearn installed):\n",
    "# bands = [(4,8),(13,30),(30,80)]\n",
    "# t_feat, feats = lfp_bandpower_features(lfp_rec, bands, win_s=0.5, step_s=0.1, duration_s=120)\n",
    "# model, states = fit_hmm_bandpower(feats, n_states=3)\n",
    "# plt.plot(t_feat, states); plt.title('LFP HMM states'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint spike–LFP–behavior features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_joint_features(spike_times_s, spike_clusters, good_units, lfp_rec, dlc_pos, dlc_time_np, bands, bin_s=0.05, duration_s=120):\n",
    "    fs = lfp_rec.get_sampling_frequency(); nbins = int(duration_s/bin_s); t_bins = np.linspace(0, duration_s, nbins+1)\n",
    "    units = np.where(good_units)[0]; spk_mat = np.zeros((nbins, len(units)))\n",
    "    for ui,u in enumerate(units):\n",
    "        st = spike_times_s[spike_clusters==u]; st = st[(st>=0)&(st<duration_s)]; counts,_ = np.histogram(st, bins=t_bins); spk_mat[:,ui] = counts/bin_s\n",
    "    lfp = lfp_rec.get_traces(start_frame=0, end_frame=min(int(duration_s*fs), lfp_rec.get_num_frames()), channel_ids=[0])[:,0]\n",
    "    band_feats=[]\n",
    "    for b in bands:\n",
    "        b_f,a_f = butter(4, [b[0]/(fs/2), b[1]/(fs/2)], btype='band')\n",
    "        lfpf = filtfilt(b_f,a_f,lfp); amp = lfpf**2; t_lfp = np.arange(len(amp))/fs\n",
    "        bp,_ = np.histogram(t_lfp, bins=t_bins, weights=amp); counts,_ = np.histogram(t_lfp, bins=t_bins); bp = bp/(counts+1e-6); band_feats.append(bp)\n",
    "    band_feats = np.vstack(band_feats).T\n",
    "    pos_interp = np.interp((t_bins[:-1]+t_bins[1:])/2, dlc_time_np, dlc_pos)\n",
    "    speed = np.concatenate([[0], np.abs(np.diff(pos_interp))/bin_s])\n",
    "    beh_feats = np.vstack([pos_interp, speed]).T\n",
    "    feats = np.hstack([spk_mat, band_feats, beh_feats])\n",
    "    return t_bins[:-1], feats, units\n",
    "\n",
    "# Example clustering (requires dlc_time_np set):\n",
    "# from sklearn.mixture import GaussianMixture\n",
    "# t_feat, feats, units = build_joint_features(spike_times, spike_clusters, good_units, lfp_rec, dlc.iloc[:,0].values, dlc_time_np, bands=[(13,30),(30,80)], bin_s=0.05, duration_s=120)\n",
    "# gm = GaussianMixture(n_components=5, covariance_type='full', n_init=3)\n",
    "# labels = gm.fit_predict(feats)\n",
    "# plt.plot(t_feat, labels); plt.title('Joint feature clusters'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional clustering of units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def cluster_place_fields(tuning_curves, n_clusters=4):\n",
    "    units = list(tuning_curves.keys())\n",
    "    if len(units)==0:\n",
    "        return None, None\n",
    "    mat = np.stack([tuning_curves[u] for u in units])\n",
    "    mat = (mat - mat.mean(axis=1, keepdims=True)) / (mat.std(axis=1, keepdims=True)+1e-6)\n",
    "    gm = GaussianMixture(n_components=n_clusters, covariance_type='full', n_init=3)\n",
    "    labels = gm.fit_predict(mat)\n",
    "    return dict(zip(units, labels)), gm\n",
    "\n",
    "# Example after compute_tuning: labels, gm = cluster_place_fields(tc, n_clusters=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-time stimulation (outline)\n",
    "- Stream spikes (threshold/template) and detect simple patterns (rate/phase-lock).\n",
    "- Trigger DAQ line for opto with safety limits.\n",
    "- Prototype on DAT-Cre, then adapt to AnxA1-Cre.\n",
    "- Implement in a real-time framework (Open Ephys plugin or C++/Python with NI-DAQ)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
