{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c9fe81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/executing/executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "/opt/anaconda3/lib/python3.12/ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/llvmlite/binding/ffi.py:136\u001b[0m, in \u001b[0;36m_lib_wrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fntab[name]\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Lazily wraps new functions as they are requested\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'LLVMPY_AddSymbol'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfindT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_tEnd_from_KS, find_tTurn_from_KS\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmy_spike_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_dataset\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_sorting_analyzer, load_sorting_analyzer\n",
      "File \u001b[0;32m/Volumes/Extreme SSD/Neuropixels/Python/pipeline/spikeinterface_waveform_extraction/my_spike_tools.py:32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# import spikeinterface as si\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspikeinterface_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msc\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspikeinterface_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfull\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msi\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_windows_from_events\u001b[39m(\n\u001b[1;32m     35\u001b[0m     stim_sec: np\u001b[38;5;241m.\u001b[39mndarray,    \n\u001b[1;32m     36\u001b[0m     tTurn_sec: \u001b[38;5;28mfloat\u001b[39m,                    \n\u001b[1;32m     37\u001b[0m     pre_stim: \u001b[38;5;28mfloat\u001b[39m,      \n\u001b[1;32m     38\u001b[0m     post_stim: \u001b[38;5;28mfloat\u001b[39m        \n\u001b[1;32m     39\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m pre_stim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_stim must be negative.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/Volumes/Extreme SSD/Neuropixels/Python/pipeline/spikeinterface_waveform_extraction/spikeinterface_v2/full.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextractors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msorters\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpostprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/Volumes/Extreme SSD/Neuropixels/Python/pipeline/spikeinterface_waveform_extraction/spikeinterface_v2/sorters/__init__.py:2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbasesorter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseSorter\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msorterlist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontainer_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContainerClient, install_package_in_container\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrunsorter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m run_sorter, run_sorter_local, run_sorter_container, read_sorter_folder\n",
      "File \u001b[0;32m/Volumes/Extreme SSD/Neuropixels/Python/pipeline/spikeinterface_waveform_extraction/spikeinterface_v2/sorters/sorterlist.py:23\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m YassSorter\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# based on spikeinertface.sortingcomponents\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspyking_circus2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Spykingcircus2Sorter\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtridesclous2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tridesclous2Sorter\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minternal\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimplesorter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleSorter\n",
      "File \u001b[0;32m/Volumes/Extreme SSD/Neuropixels/Python/pipeline/spikeinterface_waveform_extraction/spikeinterface_v2/sorters/internal/spyking_circus2.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparsity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_sparsity\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msortinganalyzer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_sorting_analyzer\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuration\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto_merge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_potential_auto_merge\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalyzer_extension_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComputeTemplates\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparsity\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelSparsity\n",
      "File \u001b[0;32m/Volumes/Extreme SSD/Neuropixels/Python/pipeline/spikeinterface_waveform_extraction/spikeinterface_v2/curation/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuration_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find_duplicated_spikes\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mremove_redundant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_redundant_units, find_redundant_units\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mremove_duplicated_spikes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_duplicated_spikes\n",
      "File \u001b[0;32m/Volumes/Extreme SSD/Neuropixels/Python/pipeline/spikeinterface_waveform_extraction/spikeinterface_v2/curation/curation_tools.py:8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspikeinterface_v2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SortingAnalyzer\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     HAVE_NUMBA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numba/__init__.py:73\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m get_versions\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m generate_version_info\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types, errors\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Re-export typeof\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numba/core/config.py:17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     _HAVE_YAML \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mllvmlite\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbinding\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mll\u001b[39;00m\n\u001b[1;32m     20\u001b[0m IS_WIN32 \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mplatform\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m IS_OSX \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mplatform\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/llvmlite/binding/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mThings that rely on the LLVM library\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdylib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecutionengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitfini\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/llvmlite/binding/dylib.py:36\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;28mstr\u001b[39m(outerr))\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# ============================================================================\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# FFI\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m ffi\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mLLVMPY_AddSymbol\u001b[38;5;241m.\u001b[39margtypes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     37\u001b[0m     c_char_p,\n\u001b[1;32m     38\u001b[0m     c_void_p,\n\u001b[1;32m     39\u001b[0m ]\n\u001b[1;32m     41\u001b[0m ffi\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mLLVMPY_SearchAddressOfSymbol\u001b[38;5;241m.\u001b[39margtypes \u001b[38;5;241m=\u001b[39m [c_char_p]\n\u001b[1;32m     42\u001b[0m ffi\u001b[38;5;241m.\u001b[39mlib\u001b[38;5;241m.\u001b[39mLLVMPY_SearchAddressOfSymbol\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m c_void_p\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/llvmlite/binding/ffi.py:139\u001b[0m, in \u001b[0;36m_lib_wrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fntab[name]\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Lazily wraps new functions as they are requested\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     cfn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib, name)\n\u001b[1;32m    140\u001b[0m     wrapped \u001b[38;5;241m=\u001b[39m _lib_fn_wrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock, cfn)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fntab[name] \u001b[38;5;241m=\u001b[39m wrapped\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/llvmlite/binding/ffi.py:131\u001b[0m, in \u001b[0;36m_lib_wrapper._lib\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lib\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# Not threadsafe.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib_handle:\n\u001b[0;32m--> 131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_lib()\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib_handle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/llvmlite/binding/ffi.py:117\u001b[0m, in \u001b[0;36m_lib_wrapper._load_lib\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _suppress_cleanup_errors(_importlib_resources_path(\n\u001b[1;32m    115\u001b[0m             \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m.\u001b[39mrpartition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    116\u001b[0m             get_library_name())) \u001b[38;5;28;01mas\u001b[39;00m lib_path:\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib_handle \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(\u001b[38;5;28mstr\u001b[39m(lib_path))\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;66;03m# Check that we can look up expected symbols.\u001b[39;00m\n\u001b[1;32m    119\u001b[0m         _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lib_handle\u001b[38;5;241m.\u001b[39mLLVMPY_GetVersionInfo()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/ctypes/__init__.py:379\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m _dlopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, mode)\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "from tkinter import Tk\n",
    "from tkinter import filedialog\n",
    "import pathlib\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "import sys\n",
    "sys.path.append(str(pathlib.Path.cwd() / \"pipeline\" / \"spikeinterface_waveform_extraction\"))\n",
    "from joblib import Parallel, delayed\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import numbers\n",
    "import warnings\n",
    "warnings.filterwarnings('default')\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import spikeinterface_v2 as si\n",
    "from DemoReadSGLXData.readSGLX import readMeta, SampRate, makeMemMapRaw, ExtractDigital\n",
    "from findT import find_tEnd_from_KS, find_tTurn_from_KS\n",
    "from collections import defaultdict\n",
    "from my_spike_tools import *\n",
    "\n",
    "from spikeinterface_v2 import download_dataset\n",
    "from spikeinterface_v2 import create_sorting_analyzer, load_sorting_analyzer\n",
    "import spikeinterface_v2.extractors as se\n",
    "\n",
    "import spikeinterface_v2.full as si\n",
    "import spikeinterface_v2.extractors as se\n",
    "import spikeinterface_v2.preprocessing as spre\n",
    "import spikeinterface_v2.sorters as ss\n",
    "import spikeinterface_v2.postprocessing as spost\n",
    "import spikeinterface_v2.qualitymetrics as sqm\n",
    "import spikeinterface_v2.comparison as sc\n",
    "import spikeinterface_v2.exporters as sexp\n",
    "import spikeinterface_v2.curation as scur\n",
    "import spikeinterface_v2.widgets as sw\n",
    "\n",
    "\n",
    "core_dir = r\"D:/Neuropixels\"\n",
    "date_strs = [\n",
    "    '01292025', '01302025', '01312025', '02012025',\n",
    "    '02022025', '02032025', '02042025', '02062025', '02072025',\n",
    "    '02092025', '02102025', '02112025', '02122025', '02132025',\n",
    "    '02142025', '02152025', '02162025'\n",
    "]\n",
    "session_names = [f\"9153_{d}_tagging_g0\" for d in date_strs]\n",
    "\n",
    "for session in session_names:\n",
    "    selected_dir = os.path.join(core_dir, session, f\"{session}_imec0\")\n",
    "    selected_dir = pathlib.Path(selected_dir)\n",
    "    base_dir = str(selected_dir)\n",
    "    print(base_dir)\n",
    "    \n",
    "    ######## load neural data ##########\n",
    "    spike_mask = np.load(os.path.join(selected_dir, \"kilosort4\\\\spike_mask.npy\"))\n",
    "    spike_seconds = np.load(os.path.join(selected_dir, \"kilosort4\\\\spike_seconds_adj.npy\"))[spike_mask]\n",
    "    spike_clusters = np.load(os.path.join(selected_dir, \"kilosort4\\\\spike_clusters.npy\"))[spike_mask]\n",
    "    spike_positions = np.load(os.path.join(selected_dir, \"kilosort4\\\\spike_positions.npy\"))[spike_mask]\n",
    "    templates = np.load(os.path.join(selected_dir, \"kilosort4\\\\templates.npy\"))\n",
    "\n",
    "\n",
    "    strobe_seconds = np.load(os.path.join(selected_dir, \"kilosort4\\\\strobe_seconds.npy\"))\n",
    "\n",
    "    tagged_good_units = np.load(os.path.join(selected_dir, \"kilosort4\\\\tagged_good_units.npy\"))\n",
    "    tagged_mua_units = np.load(os.path.join(selected_dir, \"kilosort4\\\\tagged_mua_units.npy\"))\n",
    "\n",
    "    unit_label = pd.read_csv(os.path.join(selected_dir, \"kilosort4qMetrics\\\\templates._bc_unit_labels.tsv\"), sep=\"\\t\")\n",
    "\n",
    "    ######## load behavioral data ##########\n",
    "    session_name = selected_dir.parts[-1]\n",
    "    date_str = session_name.split('_')[1]\n",
    "\n",
    "    # Convert MMDDYYYY to YYYY-MM-DD\n",
    "    date_obj = datetime.datetime.strptime(date_str, \"%m%d%Y\")\n",
    "    date_str = date_obj.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # Search for matching file\n",
    "    event_dir = pathlib.Path(r\"D:\\Neuropixels\\Event\\9153\")\n",
    "    event_file = event_dir.glob(f\"*{date_str}*.csv\")\n",
    "    events = pd.read_csv(event_file.__next__())\n",
    "    events = events.drop_duplicates(subset='Index', keep='last') # remove duplicates\n",
    "\n",
    "    dlc_dir = pathlib.Path(r\"D:\\Neuropixels\\DLC\\9153\")\n",
    "    dlc_file = dlc_dir.glob(f\"*{date_str}*.h5\")\n",
    "    dlc = pd.read_hdf(dlc_file.__next__())\n",
    "    dlc.columns = dlc.columns.droplevel(0)\n",
    "    dlc = dlc.loc[dlc.index >= events['Index'].iloc[0]] # trim DLC data to the first index in the event file\n",
    "\n",
    "    # Check the number of frames in each file\n",
    "    start = events['Index'].iloc[0] # When the first event starts\n",
    "    end = events['Index'].iloc[-1]\n",
    "    print(\"First index: \", start)\n",
    "    print(\"Last index: \", end)\n",
    "    stim = events['Stim'].to_numpy()\n",
    "    print(\"Event file length: \", len(stim))\n",
    "    print(\"Strobe file length: \", len(strobe_seconds))\n",
    "\n",
    "    snout_x = dlc[('Snout', 'x')].to_numpy()\n",
    "    snout_y = dlc[('Snout', 'y')].to_numpy()\n",
    "    print(\"DLC file length: \", len(snout_x))\n",
    "    print(\"Estimated # of frames: \", (strobe_seconds[-1] - strobe_seconds[0]) * 89.97)\n",
    "\n",
    "    # Create estimated strobe timings array by uniform interval between the first and the last strobe time stamps\n",
    "    estimated_strobe_seconds = np.linspace(strobe_seconds[0], strobe_seconds[-1], len(events))\n",
    "    print(\"Strobe file length(estimate): \", len(estimated_strobe_seconds))\n",
    "\n",
    "    ### Interpolate DLC data ###\n",
    "    from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "    # Specify body parts\n",
    "    bodyparts = dlc.columns.get_level_values(0).unique().tolist()\n",
    "    coords = dlc.columns.get_level_values(1).unique().tolist()[0:2]\n",
    "\n",
    "    # Threshold for likelihood\n",
    "    likelihood_threshold = 0.7\n",
    "\n",
    "    dlc_interpolated = dlc.copy()\n",
    "\n",
    "    # Process each body part\n",
    "    for bp in bodyparts:\n",
    "        for axis in coords:\n",
    "            series = dlc[bp][axis]\n",
    "            likelihood = dlc[bp]['likelihood']\n",
    "            \n",
    "            # Mask low-confidence data\n",
    "            series_masked = series.copy()\n",
    "            series_masked[likelihood < likelihood_threshold] = np.nan\n",
    "            \n",
    "            # Get valid indices and values\n",
    "            valid = ~series_masked.isna()\n",
    "            x_valid = np.arange(len(series_masked))[valid]\n",
    "            y_valid = series_masked[valid]\n",
    "            \n",
    "            spline = UnivariateSpline(x_valid, y_valid, k=1, s=0)\n",
    "            interpolated = spline(np.arange(len(series_masked)))\n",
    "            series_filled = series_masked.copy()\n",
    "            series_filled[~valid] = interpolated[~valid]\n",
    "            dlc_interpolated[bp][axis] = series_filled\n",
    "\n",
    "    # Kinematic analysis\n",
    "    bodyparts = ['Snout', 'Tail']\n",
    "    x_coords = np.stack([dlc_interpolated[(bp, 'x')].values for bp in bodyparts], axis=1)\n",
    "    y_coords = np.stack([dlc_interpolated[(bp, 'y')].values for bp in bodyparts], axis=1)\n",
    "    mean_x = np.mean(x_coords, axis=1)\n",
    "    mean_y = np.mean(y_coords, axis=1)\n",
    "    dt = np.median(np.diff(estimated_strobe_seconds))\n",
    "    vx = np.gradient(mean_x, dt)\n",
    "    vy = np.gradient(mean_y, dt)\n",
    "    speed = np.sqrt(vx**2 + vy**2)\n",
    "    acceleration = np.gradient(speed, dt)\n",
    "\n",
    "    # Classify the units\n",
    "    all_units = np.where(np.isin(unit_label['unitType'], [1, 2]))[0]\n",
    "    tagged_units = np.concatenate([tagged_good_units, tagged_mua_units]).squeeze()\n",
    "    good_units = np.where(unit_label['unitType'] == 1)[0]\n",
    "    mua_units = np.where(unit_label['unitType'] == 2)[0]\n",
    "\n",
    "\n",
    "    # Choose a window around each spike (e.g., +/- 1 second)\n",
    "    peri_window = 1.0  # seconds\n",
    "    frame_rate = 90  # Hz\n",
    "    n_bins = int(2 * peri_window * frame_rate) + 1\n",
    "    peri_time_axis = np.linspace(-peri_window, peri_window, n_bins)\n",
    "\n",
    "    tagged_units_final = []\n",
    "    for unit in tqdm(all_units):\n",
    "        plt.figure(figsize=(20, 7))\n",
    "        plot_times = {}\n",
    "        # --------------Plot the waveform of the unit-----------------\n",
    "        t0 = time.time()\n",
    "        plt.subplot(2,5,1)\n",
    "        beh_folder = os.path.join(selected_dir, 'analyzer_beh')\n",
    "        behavioral_analyzer = load_sorting_analyzer(folder=beh_folder, format='binary_folder')\n",
    "        beh_median_wf_mc  = np.load(os.path.join(selected_dir, \"kilosort4\\\\waveform_beh_median.npy\"), allow_pickle=True).item()[unit]\n",
    "        if beh_median_wf_mc is not None:\n",
    "            if unit in tagged_units:\n",
    "                tag_folder = os.path.join(selected_dir, 'analyzer_tag')\n",
    "                tag_analyzer = load_sorting_analyzer(folder=tag_folder, format='binary_folder')\n",
    "                tag_median_wf_mc = np.load(os.path.join(selected_dir, \"kilosort4\\\\waveform_tag_median.npy\"), allow_pickle=True).item()[unit]\n",
    "                if tag_median_wf_mc is not None:\n",
    "                    # Compute convolution similarity\n",
    "                    conv_similarity_median = compute_waveform_similarity_convolution(\n",
    "                        beh_median_wf_mc,\n",
    "                        tag_median_wf_mc,\n",
    "                        behavioral_analyzer,\n",
    "                        tag_analyzer,      \n",
    "                        unit,\n",
    "                        use_main_channel=True,\n",
    "                    )\n",
    "                    plt.plot(beh_median_wf_mc, color='k')\n",
    "                    plt.plot(tag_median_wf_mc, color='blue', alpha=0.5)\n",
    "                    plt.gca().invert_yaxis()  # Invert y-axis for waveform\n",
    "                    plt.title(f'Waveform')\n",
    "                    plt.xlabel('Time (samples)')\n",
    "                    plt.ylabel('Template used for waveform extraction')\n",
    "                    plt.legend(['Behavioral', 'Tagged'], loc='upper right')\n",
    "                    plt.text(0.5, 0.80, f'Conv similarity: {conv_similarity_median:.2f}', transform=plt.gca().transAxes, fontsize=10, verticalalignment='top')\n",
    "                    plt.tight_layout()\n",
    "                    if conv_similarity_median > 0.8:\n",
    "                        tagged_units_final.append(unit)\n",
    "            else:\n",
    "                plt.plot(beh_median_wf_mc, color='k')\n",
    "                plt.gca().invert_yaxis()  # Invert y-axis for waveform\n",
    "                plt.title(f'Waveform')\n",
    "                plt.xlabel('Time (samples)')\n",
    "                plt.ylabel('Waveform(median)')\n",
    "                plt.tight_layout()\n",
    "        else:\n",
    "            plt.text(0.5, 0.5, 'No waveform data available', horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
    "        plot_times['waveform'] = time.time() - t0\n",
    "\n",
    "        #-------------plot the position on the probe and waveform of the unit------------------\n",
    "        t0 = time.time()\n",
    "        plt.subplot(2,5,2)\n",
    "        shank_width = 24       # in microns\n",
    "        shank_length = -5000   # in microns\n",
    "        shank_spacing = 250    # center-to-center in microns\n",
    "        n_shanks = 4\n",
    "        for i in range(n_shanks):\n",
    "            center_x = (i-3) * shank_spacing\n",
    "            left = center_x - shank_width / 2\n",
    "            bottom = 0\n",
    "            rect = plt.Rectangle((left, bottom), shank_width, shank_length,\n",
    "                                linewidth=2, edgecolor='k', facecolor='none', zorder=1)\n",
    "            plt.gca().add_patch(rect)\n",
    "        plt.xlabel('Lateral (μm) 0 = ML:-1500μm')\n",
    "        plt.ylabel('Depth (μm) 0 = brain surface')\n",
    "        plt.title('Neuropixels 2.0 4-shank Probe Layout')\n",
    "        unit_position = spike_positions[spike_clusters == unit]\n",
    "        plt.scatter(np.median(unit_position[:,0])-800, np.median(unit_position[:,1])-5000, s=10, c='red')\n",
    "        plt.gca().invert_xaxis()\n",
    "        plot_times['probe_layout'] = time.time() - t0\n",
    "\n",
    "        # ---------------plot peri-spike speed (mean across all spikes)-----------------\n",
    "        t0 = time.time()\n",
    "        unit_spike_times = spike_seconds[spike_clusters == unit]\n",
    "        # Vectorized: find closest strobe indices for all spikes\n",
    "        spike_indices = np.searchsorted(estimated_strobe_seconds, unit_spike_times, side='left')\n",
    "        spike_indices = np.clip(spike_indices, 0, len(estimated_strobe_seconds)-1)\n",
    "        peri_idx = np.arange(-90, 91)  # 181 frames, for 1s window at 90Hz\n",
    "        peri_speed = []\n",
    "        valid_spikes = []\n",
    "        for idx in spike_indices:\n",
    "            idxs = idx + peri_idx\n",
    "            if idxs[0] < 0 or idxs[-1] >= len(speed):\n",
    "                continue\n",
    "            peri_speed.append(speed[idxs])\n",
    "            valid_spikes.append(idx)\n",
    "        if len(peri_speed) == 0:\n",
    "            print(f\"Skipping unit {unit}: no peri-event speed data available.\")\n",
    "            continue\n",
    "        peri_speed_stack = np.vstack(peri_speed)\n",
    "        plt.subplot(2,5,3)\n",
    "        plt.plot(peri_time_axis, np.nanmean(peri_speed_stack, axis=0), color='gray', alpha=0.8)\n",
    "        plt.xlabel('Time from spike (s)')\n",
    "        plt.ylabel('Speed')\n",
    "        plt.ylim(100,1100)\n",
    "        plt.title(f'Unit #{unit}: Mean speed around tagged unit spikes')\n",
    "        plot_times['peri_speed'] = time.time() - t0\n",
    "\n",
    "        plt.subplot(2,5,4)\n",
    "        plt.subplot(2,5,5)\n",
    "        # ----------------scatter plot of the spike on the arena-----------------\n",
    "        t0 = time.time()\n",
    "        # Vectorized: get snout positions for all valid spikes\n",
    "        fired_coords = np.column_stack((snout_x[spike_indices], snout_y[spike_indices]))\n",
    "        plt.subplot(2,5,6)\n",
    "        plt.scatter(fired_coords[:, 0], fired_coords[:, 1], s=1, color='black', alpha=0.5)\n",
    "        plt.xlim(200, 1230)\n",
    "        plt.ylim(40, 1070)\n",
    "        plt.ylabel('Y coordinate')\n",
    "        plt.title('Snout position when the unit fired')\n",
    "        plot_times['scatter_arena'] = time.time() - t0\n",
    "\n",
    "        # ----------- Plot firing rate heatmap------------------\n",
    "        t0 = time.time()\n",
    "        x_bins = np.linspace(200, 1230, 15)\n",
    "        y_bins = np.linspace(40, 1070, 15)\n",
    "        spike_hist, xedges, yedges = np.histogram2d(fired_coords[:, 0], fired_coords[:, 1], bins=[x_bins, y_bins])\n",
    "        occupancy_hist, _, _ = np.histogram2d(snout_x, snout_y, bins=[x_bins, y_bins])\n",
    "        occupancy_sec = occupancy_hist / frame_rate\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            firing_rate_map = np.where(spike_hist >= 5, spike_hist / occupancy_sec, np.nan)\n",
    "        plt.subplot(2,5,7)\n",
    "        im = plt.imshow(np.rot90(firing_rate_map), extent=[x_bins[0], x_bins[-1], y_bins[0], y_bins[-1]], aspect='auto', cmap='coolwarm')\n",
    "        plt.xlabel('X coordinate')\n",
    "        plt.title('Firing Rate Heatmap')\n",
    "        plt.colorbar(im, label='Hz')\n",
    "        plt.title('Firing Rate Heatmap')\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        plot_times['firing_rate_heatmap'] = time.time() - t0\n",
    "\n",
    "        # ----------- Plot mean speed vectors at spikes------------------        \n",
    "        t0 = time.time()\n",
    "        x_bins = np.linspace(200, 1230, 10)\n",
    "        y_bins = np.linspace(40, 1070, 10)\n",
    "        x_centers = (x_bins[:-1] + x_bins[1:]) / 2\n",
    "        y_centers = (y_bins[:-1] + y_bins[1:]) / 2\n",
    "        # Vectorized binning\n",
    "        sx = snout_x[spike_indices]\n",
    "        sy = snout_y[spike_indices]\n",
    "        svx = vx[spike_indices]\n",
    "        svy = vy[spike_indices]\n",
    "        xi = np.digitize(sx, x_bins) - 1\n",
    "        yi = np.digitize(sy, y_bins) - 1\n",
    "        valid = (xi >= 0) & (xi < len(x_centers)) & (yi >= 0) & (yi < len(y_centers))\n",
    "        vx_spike_grid = np.zeros((len(x_centers), len(y_centers)))\n",
    "        vy_spike_grid = np.zeros((len(x_centers), len(y_centers)))\n",
    "        spike_count_grid = np.zeros((len(x_centers), len(y_centers)))\n",
    "        np.add.at(vx_spike_grid, (xi[valid], yi[valid]), svx[valid])\n",
    "        np.add.at(vy_spike_grid, (xi[valid], yi[valid]), svy[valid])\n",
    "        np.add.at(spike_count_grid, (xi[valid], yi[valid]), 1)\n",
    "        with np.errstate(invalid='ignore', divide='ignore'):\n",
    "            vx_spike_mean = np.where(spike_count_grid >= 5, vx_spike_grid / spike_count_grid, np.nan)\n",
    "            vy_spike_mean = np.where(spike_count_grid >= 5, vy_spike_grid / spike_count_grid, np.nan)\n",
    "        X, Y = np.meshgrid(x_centers, y_centers, indexing='ij')\n",
    "        mask = ~np.isnan(vx_spike_mean) & ~np.isnan(vy_spike_mean)\n",
    "        plt.subplot(2,5,8)\n",
    "        plt.quiver(X[mask], Y[mask], vx_spike_mean[mask], vy_spike_mean[mask], angles='xy', scale_units='xy', scale=2, color='red', width=0.008)\n",
    "        plt.xlabel('X coordinate')\n",
    "        plt.ylabel('Y coordinate')\n",
    "        plt.title('Mean Speed Vectors at Spikes (min 10 spikes/bin)')\n",
    "        plt.xlim(200, 1230)\n",
    "        plt.ylim(40, 1070)\n",
    "        plt.gca().set_aspect('equal', adjustable='box')\n",
    "        plot_times['mean_speed_vectors'] = time.time() - t0\n",
    "\n",
    "        plt.subplot(2,5,9)\n",
    "        plt.subplot(2,5,10)\n",
    "\n",
    "        # Ensure the EDA directory exists before saving\n",
    "        if unit in tagged_units:\n",
    "            if unit in good_units:\n",
    "                prefix = \"tagged_good\"\n",
    "            elif unit in mua_units:\n",
    "                prefix = \"tagged_mua\"\n",
    "        else:\n",
    "            if unit in good_units:\n",
    "                prefix = \"good\"\n",
    "            elif unit in mua_units:\n",
    "                prefix = \"mua\"\n",
    "        eda_dir = selected_dir / \"EDA\"\n",
    "        eda_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(str(eda_dir / f\"{prefix}_unit{unit}_EDA.png\"), dpi=300)\n",
    "        # plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # At the end, print timing for this unit\n",
    "        # print(f\"Plot times for unit {unit}: {plot_times}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4aa9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
